---
layout: post
title:  "Что такое Unicode"
date:   2019-09-02 00:20:14 +0600
categories: jekyll update
description: немного о Юникоде
---

## Что же такое Unicode и кодировки?
---
Многие программисты, а также люди, имеющие то или иное отношение к работе с компьютером, не понимают что такое Unicode, кодировки, что, тем не менее, не мешает им успешно (а иногда и не очень) ими пользоваться.
Мы попытаемся максимально просто разобраться с этими понятиями и узнать как работает Юникод. 

На самом деле каждый человек в современном мире, взаимодействующий с интернетом, постоянно использует продукт интеллектуального труда трех гениальных основателей стандарта Юникод. Уже в 1987 году началась работа по созданию архитектуры Юникода силами трех человек: Ли Коллинза, Джо Беккера и Марка Дэвиса. 
  ![Image](https://i.imgur.com/6wOV1W4.jpg)

*<center>На фотографии (слева на право): Марк Дэвис и Ли Коллинз празднуют двацатипятилетие консорциума Unicode в 2016 году.</center>*

Так что же нам дали эти три человека и почему Юникод стал настолько популярен? 

Unicode представляет собой общепринятый мировой стандарт кодирования, от букв алфавита до математических символов. 
Возникает вопрос, зачем символы, буквы, смайлики, знаки кодировать, неужели их нельзя просто использовать как есть?
Компьютер воспринимает только биты и байты, для него не существует букв или знаков препинания в том виде, в котором мы привыкли их воспринимать. Поэтому букву или символ в электронной почте, или программе мы видим уже в расшифрованном виде, передаются и хранятся они в закодированном виде.  
Кодировка это способ хранения информации, которую вы хотите передать.
Например, английская буква ```A```, передается в виде ```0100 0001```. Весит эта буква ровно один байт и состоит, как вы видите, из 8 бит. 
Но чаще всего мы видим закодированные буквы в шестнадцатеричной системе счисления (HEX), где, помимо цифр, используются буквенные значения:

```
0 1 2 3 4 5 6 7 8 9 A B C D E F
```
Давайте попробуем закодировать слово «Documentat.io» в кодировке ASCII, воспользовавшись таблицей:
![Image](https://upload.wikimedia.org/wikipedia/commons/thumb/4/4f/ASCII_Code_Chart.svg/1200px-ASCII_Code_Chart.svg.png)


Наше слово будет выглядеть как:
```
44 6F 63 75 6D 65 6E 74 61 74 2E 69 6F
```

Когда компьютер считывает данные, которые вы хотите передать, он обязательно должен понимать какую кодировку вы использовали, иначе вы столкнетесь с тем, что увидите непереводимые на нормальный язык символы, так называемые «кракозябры». 
```
ЊСЃСЏ. Р’РѕС‚ СЃРєРѕР»СЊРєРѕ РѕРЅР
```
Наверняка вы с такими символами встречались и не раз.
## Несколько проблем и одно решение.
---
В те времена, когда еще не было интернета, разные страны придумывали свои кодировки. Для многих систем и компьютеров в западных странах была повсеместно распространена кодировка ASCII «American Standard Code for Information Interchange». Во времена до Юникода для многих она являлась стандартом. Япония, СССР и другие страны разрабатывали свои уникальные кодировки. Именно поэтому их сейчас большое разнообразие. 

Все это «работало», но чтобы, например, прочитать письмо из Японии тебе необходимо было установить на компьютер кодировку, в которой писалось письмо, и нет никакой гарантии, что система сможет ее поддерживать. Решение было и люди использовали факсы.
![Image](https://i.imgur.com/f0YAyG6.jpg)

Отсканировать и передать лист в точно таком же виде, просто и удобно. 

Но наступала эпоха мировой паутины, и это несло за собой ряд проблем, которые требовали лаконичного и универсального решения. 
Давайте сформулируем эти проблемы: 
1. *Универсальность.* 
Стандарт обязательно должен был включать в себя множество алфавитов и символов, принятых в разных странах мира.  А в связи с многочисленностью закодированных символов возникает проблема номер 2. 
2. *Хранение.* 
Для кодирования большого количества букв и символов должно быть большим и кодовое пространство.
3. *Совместимость.* 
Новый стандарт должен поддерживать старую кодировку ASCII, при этом не просто поддерживать, а кодировать идентично, для работы устаревших систем.
4. *Нули.* 
В закодированном символе не должно быть 8 нулей подряд, это связано с тем, что многочисленные устаревшие системы воспринимали это как конец передачи и прекращали дальнейший процесс считывания данных. Для них строка заканчивалась.

Решением возникших проблем стал стандарт, предложенный некоммерческой организацией «Консорциум Юникода».
Их кодировка UTF-8 решала все эти проблемы. 

1. Диапазон `0 - 007F` был выделен для устаревшего ASCII
2. Диапазон выше `007F` идет в виде определенной последовательности байт
3. Вначале всегда идет счетчик указывающий на количество байт в кодированном символе, счетчик начинается с единиц:
`110x xxxx` - две еденицы указывают на 2 байта в последовательности (включая себя самого), второй байт начинается с `10xx xxxx`, где 10 указывает на продолжение кодированного символа
4. Если значение кодируется 3 байтами, то это будет выглядеть так:
 `1110 xxxx` `10xx xxxx` `10xx xxxx`
5. Такая уникальная и, в тоже время, простая идея решила проблему нулей в строке, их просто не может быть 8 подряд.  
6. Более одного миллиона возможных кодированных символов(!).

Юникод можно сравнить с легендой о строительстве Вавилонской башни, только если бог заставил людей говорить на разных языках, юникод же наоборот объединяет разные языки в один универсальный. 
В UTF-8 каждая кодовая позиция от 0 до 127 хранится в одном байте, все последующие в 2, 3 и т.д до 6 байт.
 Давайте попробуем закодировать слово `«Documentat.io»`, но теперь в юникоде:

```
U+0044 U+006F U+0063 U+0075 U+006D U+0065 U+006E U+0074 U+0061 U+0074 U+002E U+0069 U+006F
```
Это же слово можно представить и в таком виде:
```
U+4400 U+6F00 U+6300 U+7500 U+6D00 U+6500 U+6E00 U+7400 U+6100 U+7400 U+2E00 U+6900 U+6F00
```
Слово от этого не изменится, но меняется *порядок хранения байтов* в юникоде. Для того чтобы система понимала в каком порядке их читать, были придуманы специальные *метки*: `FE FF` и `FF EE`, эти метки в начале каждой строки говорят компьютеру в каком порядке читать эту строку, метки называют BOM (Byte Order Mark).

## Что же выбрать UTF-8 или UTF-16? 
---
Теперь, когда нам стало понятнее что представляет из себя кодировка UTF-8, не составит труда разобраться в кодировке UTF-16. UTF-8 назван так, потому что использует минимум восемь бит (один байт) для хранения кодированного символа. 
Соответственно UTF-16 использует минимум шестнадцать бит (или два байта) для хранения закодированного символа. Это увеличивает память для хранения и передачи данных, но у UTF-16 есть свой ряд преимуществ:
* Используете язык (алфавит), который невозможно уместить в 1 байт - выберите UTF-16.
* Java, JavaScript базируются на UTF-16
* Внутренний формат библиотек Windows также базируется на UTF-16, в связи с чем лучше совместимость с системами Windows.

Наравне с преимуществами у кодировки UTF-16 есть и свои недостатки:
* Больший размер для хранения данных
* Много нулевых байтов в диапазоне `0 - 007F` 
* Худшая совместимость с Unix-системами, в сравнении с UTF-8
* Возможность изменения порядка байт UTF-16LE (little endian) и UTF-16BE (big endian)

Преимущества UTF-8:
* Отсутствие нулевых байтов
* Лучше себя проявляет для текстового хранения и в работе с протоколами сети.
* Отличная совместимость с кодировкой ASCII и как следствие меньший объем данных при использовании английского языка.
* Отсутствие зависимости от порядка байт
* Хорошая совместимость с Unix.

Недостатки UTF-8:
* Занимает больше места чем UTF-16 в языках использующих для хранения 2 байта
* Более замедленное индексирование в памяти в сравнении с UTF-16. 

Тем не менее, всегда следует учитывать среду в которой вы работаете и использовать рекомендуемую для нее кодировку. 