I"F9<h2 id="что-же-такое-unicode-и-кодировки">Что же такое Unicode и кодировки?</h2>
<hr />
<p>Многие программисты, а также люди, имеющие то или иное отношение к работе с компьютером, не понимают что такое Unicode, кодировки, что, тем не менее, не мешает им успешно (а иногда и не очень) ими пользоваться.
Мы попытаемся максимально просто разобраться с этими понятиями и узнать как работает Юникод.</p>

<p>На самом деле каждый человек в современном мире, взаимодействующий с интернетом, постоянно использует продукт интеллектуального труда трех гениальных основателей стандарта Юникод. Уже в 1987 году началась работа по созданию архитектуры Юникода силами трех человек: Ли Коллинза, Джо Беккера и Марка Дэвиса. 
  <img src="https://i.imgur.com/6wOV1W4.jpg" alt="Image" /></p>

<p><em><center>На фотографии (слева на право): Марк Дэвис и Ли Коллинз празднуют двацатипятилетие консорциума Unicode в 2016 году.</center></em></p>

<p>Так что же нам дали эти три человека и почему Юникод стал настолько популярен?</p>

<p>Unicode представляет собой общепринятый мировой стандарт кодирования, от букв алфавита до математических символов. 
Возникает вопрос, зачем символы, буквы, смайлики, знаки кодировать, неужели их нельзя просто использовать как есть?
Компьютер воспринимает только биты и байты, для него не существует букв или знаков препинания в том виде, в котором мы привыкли их воспринимать. Поэтому букву или символ в электронной почте, или программе мы видим уже в расшифрованном виде, передаются и хранятся они в закодированном виде.<br />
Кодировка это способ хранения информации, которую вы хотите передать.
Например, английская буква <code class="highlighter-rouge">A</code>, передается в виде <code class="highlighter-rouge">0100 0001</code>. Весит эта буква ровно один байт и состоит, как вы видите, из 8 бит. 
Но чаще всего мы видим закодированные буквы в шестнадцатеричной системе счисления (HEX), где, помимо цифр, используются буквенные значения:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0 1 2 3 4 5 6 7 8 9 A B C D E F
</code></pre></div></div>
<p>Давайте попробуем закодировать слово «Documentat.io» в кодировке ASCII, воспользовавшись таблицей:
<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/4/4f/ASCII_Code_Chart.svg/1200px-ASCII_Code_Chart.svg.png" alt="Image" /></p>

<p>Наше слово будет выглядеть как:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>44 6F 63 75 6D 65 6E 74 61 74 2E 69 6F
</code></pre></div></div>

<p>Когда компьютер считывает данные, которые вы хотите передать, он обязательно должен понимать какую кодировку вы использовали, иначе вы столкнетесь с тем, что увидите непереводимые на нормальный язык символы, так называемые «кракозябры».</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ЊСЃСЏ. Р’РѕС‚ СЃРєРѕР»СЊРєРѕ РѕРЅР
</code></pre></div></div>
<p>Наверняка вы с такими символами встречались и не раз.</p>
<h2 id="несколько-проблем-и-одно-решение">Несколько проблем и одно решение.</h2>
<hr />
<p>В те времена, когда еще не было интернета, разные страны придумывали свои кодировки. Для многих систем и компьютеров в западных странах была повсеместно распространена кодировка ASCII «American Standard Code for Information Interchange». Во времена до Юникода для многих она являлась стандартом. Япония, СССР и другие страны разрабатывали свои уникальные кодировки. Именно поэтому их сейчас большое разнообразие.</p>

<p>Все это «работало», но чтобы, например, прочитать письмо из Японии тебе необходимо было установить на компьютер кодировку, в которой писалось письмо, и нет никакой гарантии, что система сможет ее поддерживать. Решение было и люди использовали факсы.
<img src="https://i.imgur.com/f0YAyG6.jpg" alt="Image" /></p>

<p>Отсканировать и передать лист в точно таком же виде, просто и удобно.</p>

<p>Но наступала эпоха мировой паутины, и это несло за собой ряд проблем, которые требовали лаконичного и универсального решения. 
Давайте сформулируем эти проблемы:</p>
<ol>
  <li><em>Универсальность.</em> 
Стандарт обязательно должен был включать в себя множество алфавитов и символов, принятых в разных странах мира.  А в связи с многочисленностью закодированных символов возникает проблема номер 2.</li>
  <li><em>Хранение.</em> 
Для кодирования большого количества букв и символов должно быть большим и кодовое пространство.</li>
  <li><em>Совместимость.</em> 
Новый стандарт должен поддерживать старую кодировку ASCII, при этом не просто поддерживать, а кодировать идентично, для работы устаревших систем.</li>
  <li><em>Нули.</em> 
В закодированном символе не должно быть 8 нулей подряд, это связано с тем, что многочисленные устаревшие системы воспринимали это как конец передачи и прекращали дальнейший процесс считывания данных. Для них строка заканчивалась.</li>
</ol>

<p>Решением возникших проблем стал стандарт, предложенный некоммерческой организацией «Консорциум Юникода».
Их кодировка UTF-8 решала все эти проблемы.</p>

<ol>
  <li>Диапазон <code class="highlighter-rouge">0 - 007F</code> был выделен для устаревшего ASCII</li>
  <li>Диапазон выше <code class="highlighter-rouge">007F</code> идет в виде определенной последовательности байт</li>
  <li>Вначале всегда идет счетчик указывающий на количество байт в кодированном символе, счетчик начинается с единиц:
<code class="highlighter-rouge">110x xxxx</code> - две еденицы указывают на 2 байта в последовательности (включая себя самого), второй байт начинается с <code class="highlighter-rouge">10xx xxxx</code>, где 10 указывает на продолжение кодированного символа</li>
  <li>Если значение кодируется 3 байтами, то это будет выглядеть так:
 <code class="highlighter-rouge">1110 xxxx</code> <code class="highlighter-rouge">10xx xxxx</code> <code class="highlighter-rouge">10xx xxxx</code></li>
  <li>Такая уникальная и, в тоже время, простая идея решила проблему нулей в строке, их просто не может быть 8 подряд.</li>
  <li>Более одного миллиона возможных кодированных символов(!).</li>
</ol>

<p>Юникод можно сравнить с легендой о строительстве Вавилонской башни, только если бог заставил людей говорить на разных языках, юникод же наоборот объединяет разные языки в один универсальный. 
В UTF-8 каждая кодовая позиция от 0 до 127 хранится в одном байте, все последующие в 2, 3 и т.д до 6 байт.
 Давайте попробуем закодировать слово <code class="highlighter-rouge">«Documentat.io»</code>, но теперь в юникоде:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>U+0044 U+006F U+0063 U+0075 U+006D U+0065 U+006E U+0074 U+0061 U+0074 U+002E U+0069 U+006F
</code></pre></div></div>
<p>Это же слово можно представить и в таком виде:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>U+4400 U+6F00 U+6300 U+7500 U+6D00 U+6500 U+6E00 U+7400 U+6100 U+7400 U+2E00 U+6900 U+6F00
</code></pre></div></div>
<p>Слово от этого не изменится, но меняется <em>порядок хранения байтов</em> в юникоде. Для того чтобы система понимала в каком порядке их читать, были придуманы специальные <em>метки</em>: <code class="highlighter-rouge">FE FF</code> и <code class="highlighter-rouge">FF EE</code>, эти метки в начале каждой строки говорят компьютеру в каком порядке читать эту строку, метки называют BOM (Byte Order Mark).</p>

<h2 id="что-же-выбрать-utf-8-или-utf-16">Что же выбрать UTF-8 или UTF-16?</h2>
<hr />
<p>Теперь, когда нам стало понятнее что представляет из себя кодировка UTF-8, не составит труда разобраться в кодировке UTF-16. UTF-8 назван так, потому что использует минимум восемь бит (один байт) для хранения кодированного символа. 
Соответственно UTF-16 использует минимум шестнадцать бит (или два байта) для хранения закодированного символа. Это увеличивает память для хранения и передачи данных, но у UTF-16 есть свой ряд преимуществ:</p>
<ul>
  <li>Используете язык (алфавит), который невозможно уместить в 1 байт - выберите UTF-16.</li>
  <li>Java, JavaScript базируются на UTF-16</li>
  <li>Внутренний формат библиотек Windows также базируется на UTF-16, в связи с чем лучше совместимость с системами Windows.</li>
</ul>

<p>Наравне с преимуществами у кодировки UTF-16 есть и свои недостатки:</p>
<ul>
  <li>Больший размер для хранения данных</li>
  <li>Много нулевых байтов в диапазоне <code class="highlighter-rouge">0 - 007F</code></li>
  <li>Худшая совместимость с Unix-системами, в сравнении с UTF-8</li>
  <li>Возможность изменения порядка байт UTF-16LE (little endian) и UTF-16BE (big endian)</li>
</ul>

<p>Преимущества UTF-8:</p>
<ul>
  <li>Отсутствие нулевых байтов</li>
  <li>Лучше себя проявляет для текстового хранения и в работе с протоколами сети.</li>
  <li>Отличная совместимость с кодировкой ASCII и как следствие меньший объем данных при использовании английского языка.</li>
  <li>Отсутствие зависимости от порядка байт</li>
  <li>Хорошая совместимость с Unix.</li>
</ul>

<p>Недостатки UTF-8:</p>
<ul>
  <li>Занимает больше места чем UTF-16 в языках использующих для хранения 2 байта</li>
  <li>Более замедленное индексирование в памяти в сравнении с UTF-16.</li>
</ul>

<p>Тем не менее, всегда следует учитывать среду в которой вы работаете и использовать рекомендуемую для нее кодировку.</p>
:ET